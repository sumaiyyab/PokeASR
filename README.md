# PokeASR
An ASR to transcribe poke orders using the Hidden Markov Model Toolkit (HTK). Final project for LING 124 @ SJSU

## Why Poke?
There are a few benefits to choosing poke as a speech recognition problem. Firstly, the structured way of ordering poke (first selecting a bowl base, then moving down to proteins, and then continuing to move down station by station) allows for the creation of a very clear and easy to formulate grammar. Secondly, since the amount and variety of available toppings is left up to the discretion of the store owner, I could create a lexicon with fewer options per station. Since this is my first work with HTK, a concise grammar and relatively smaller grammar would let me become comfortable with the toolkit without overwhelming myself.

## Process
To implement this ASR, I primarily followed the tutorial labs provided by Dr. Hahn Koo and the HTK textbook’s tutorial chapter on creating a HMM-based speech recognizer. 

While a lot of the files were automatically generated by HTK, there was some initial legwork to do. For example, I manually wrote the grammar in poke.grammar and populated the word list in poke.list. The dictionary in poke.dict was based off the BEEP pronunciation dictionary, but was adapted to account for the phonetic differences between British and American English, as well as to include Japanese loanwords that might be offerered as poke ingredients but not be included in the BEEP dictionary.

I also collected speech data for the training and testing data. The training data consists of 120 .wav files of 6 individuals speaking the same set of 20 sentences, thus allowing the HMM to get an idea of how sentences might be said by different voices. While the training does vary in age as it ranges between subjects between 9 - 40+ years old, it is more heavily skewed to the female side and thus might lead to a bias towards higher pitched voices. For privacy reasons, these .wav files have not been uploaded to GitHub.

The testing data was more restricted, and used only one speaker saying 10 sentences. 
While there were lots of little errors and obstacles here and there, such as getting the format of the MLF to align with the .wav file names, I was ultimately able to build the HMM and ran it through five times to build up results. 


## Results + Plans for the future:
I was able to evaluate my ASR using HVite and HResults and unfortunately I got some mixed results.
![results](https://drive.google.com/uc?export=view&id=1RWESGYwFWa1PTwopxVlMow92K7sL5kfA)

While I was happy with my word accuracy of 95%, my sentence accuracy leaves a lot to be desired. As I examined my output versus results I noticed that while there were a few substitution errors (e.g. “GREEN-ONION” transcribed as “ONION”), the bulk of my errors were insertion and deletion errors. Thought I am submitting this as a school project, I hope to continue improving this and lessening some of those errors. 

While I’m not yet sure how to combat the deletion errors, I believe that the insertion errors could be greatly improved through the implementation of a language model. Currently, the way the grammar is structured, any of the non-base words (i.e. anything except “salad,” “brown rice,” or “white rice”) can appear between 1 - 3 times in a sentence and possibly contain duplicates. While the appearance number is fine, in natural language people don’t order bowls by repeating the ingredient. To avoid unnatural prompts from word duplication, I used a Python script prompts2mlf.py that eliminates duplicates that occur up to 2 words ahead. Unfortunately, this script only affected the outputted training prompts/MLF, so the HMM saw no issues in allowing duplicate words. Hopefully, the addition of a language model would teach the HMM that duplicate proteins/toppings are not a natural part of this language. 
